

# Projeto de Engenharia de Dados: ETL de Fundos de Investimento da CVM com Spark e AWS simulado.

Este projeto Ã© um pipeline de dados completo para extrair, transformar e carregar (ETL) os informes diÃ¡rios de fundos de investimento brasileiros, disponibilizados publicamente pela CVM.

## ğŸ“œ Sobre o Projeto
O projeto nasceu da uniÃ£o de duas Ã¡reas de grande interesse: o mercado financeiro e a engenharia de dados. O objetivo principal foi construir um pipeline de dados robusto e escalÃ¡vel para processar um grande volume de informaÃ§Ãµes diÃ¡rias dos fundos de investimento brasileiros ao longo do perÃ­odo de 2022 atÃ© os dias atuais, disponibilizadas pela ComissÃ£o de Valores MobiliÃ¡rios (CVM).

O pipeline automatiza todo o processo, desde o download dos dados brutos atÃ© a disponibilizaÃ§Ã£o em um banco de dados relacional, pronto para anÃ¡lises e visualizaÃ§Ãµes.

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o
A soluÃ§Ã£o foi desenhada em um fluxo de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carregamento) e totalmente conteinerizada com Docker para garantir portabilidade e um ambiente de desenvolvimento isolado.

ğŸ”¹ 1. ExtraÃ§Ã£o (Extraction)
Fonte de Dados: Informes diÃ¡rios da CVM (valor da cota, PatrimÃ´nio LÃ­quido, aplicaÃ§Ãµes e resgates) a partir de 2022.

Tecnologia: Scripts em Python utilizando as bibliotecas requests para o download dos arquivos e boto3 para a comunicaÃ§Ã£o com o S3.

Armazenamento Bruto (Raw): Os dados sÃ£o salvos em um bucket no Amazon S3, que Ã© simulado localmente atravÃ©s do LocalStack.

ğŸ”¹ 2. TransformaÃ§Ã£o (Transformation)
Desafio: Processar um grande volume de dados, que ultrapassa 22 milhÃµes de registros.

Framework: Apache Spark (via PySpark) foi escolhido por sua capacidade de processamento distribuÃ­do e alta performance.

Processos:

Limpeza e padronizaÃ§Ã£o dos dados.

Tratamento de valores nulos e tipos de dados.

Enriquecimento com novas mÃ©tricas, como o cÃ¡lculo de PnL (Profit and Loss) e variaÃ§Ã£o diÃ¡ria da cota.

ğŸ”¹ 3. Carregamento (Loading)
Destino: Os dados limpos e processados sÃ£o carregados em um banco de dados PostgreSQL.

PropÃ³sito: Disponibilizar os dados estruturados para consumo por ferramentas de BI, dashboards ou outras aplicaÃ§Ãµes analÃ­ticas.

## ğŸ› ï¸ Tecnologias Utilizadas

-  Python:	Linguagem principal para scripts de extraÃ§Ã£o e transformaÃ§Ã£o.
-  Apache Spark:	Framework de processamento de dados distribuÃ­do para a etapa de transformaÃ§Ã£o.
-  PostgreSQL:	Banco de dados relacional para armazenar os dados tratados.
-  Amazon S3 (LocalStack):	Armazenamento dos dados brutos (Data Lake).
-  Docker & Docker Compose:	ConteinerizaÃ§Ã£o e orquestraÃ§Ã£o de toda a infraestrutura do projeto.


## ğŸ’» Estrutura de Pastas

```bash
ETL_CVM/
â”œâ”€â”€ app/                 # Scripts principais do projeto
â”œâ”€â”€ jars/                # Bibliotecas e drivers Java para Spark
â”œâ”€â”€ notebooks/           # Notebooks para anÃ¡lise e testes
â”œâ”€â”€ sql/                 # Scripts SQL para banco de dados
â”œâ”€â”€ src/                 # CÃ³digo fonte do ETL
â”œâ”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Arquitetura ETL.png  # Diagrama da arquitetura ETL
â”œâ”€â”€ README.md            # DocumentaÃ§Ã£o do projeto
â””â”€â”€ .gitignore           # Arquivos ignorados pelo Git
```

## ğŸš€ Como Executar o Projeto
Siga os passos abaixo para executar todo o pipeline de ETL localmente.

PrÃ©-requisitos
Antes de comeÃ§ar, garanta que vocÃª tenha as seguintes ferramentas instaladas:

Git
Docker

Passo a Passo
1. Clone o RepositÃ³rio


```bash
git clone https://github.com/jonathannovs/ETL_CVM.git
cd ETL_CVM
```

2. Inicie os ContÃªineres
Este comando irÃ¡ construir e iniciar todos os serviÃ§os definidos no docker-compose.yml (Spark, PostgreSQL, etc.) em segundo plano.

```bash
docker-compose up -d
```

3. Instale as DependÃªncias Python
Acesse o contÃªiner do Spark Master para instalar as bibliotecas Python necessÃ¡rias.

```bash
docker exec -it spark-master pip install -r /app/requirements.txt
```

4. Execute o Pipeline ETL
Execute o script principal main.py usando spark-submit. Este comando submete a aplicaÃ§Ã£o para o cluster Spark.

```bash
docker exec -it spark-master spark-submit \
  --jars /opt/bitnami/spark/jars-custom/postgresql-42.6.0.jar \
  --driver-class-path /opt/bitnami/spark/jars-custom/postgresql-42.6.0.jar \
  --master spark://spark-master:7077 \
  /app/main.py
```

O pipeline comeÃ§arÃ¡ a extrair, transformar e carregar os dados. O processo pode levar alguns minutos dependendo do volume de dados e da performance da sua mÃ¡quina.

5. Verifique os Dados no PostgreSQL
ApÃ³s a execuÃ§Ã£o do pipeline, vocÃª pode se conectar ao banco de dados para verificar se os dados foram carregados corretamente.


```bash SQL
SELECT * FROM cvm.fundos LIMIT 10;
```

## ğŸ”® PrÃ³ximos Passos
O roadmap futuro deste projeto inclui a migraÃ§Ã£o da soluÃ§Ã£o local para uma arquitetura 100% serverless na nuvem AWS, visando maior escalabilidade, resiliÃªncia e automaÃ§Ã£o.

[ ] Data Lake: Utilizar o Amazon S3 como Data Lake definitivo para os dados brutos e processados.

[ ] Processamento: Substituir o cluster Spark local por AWS Glue ou Amazon EMR para processamento distribuÃ­do sob demanda.

[ ] Data Warehouse: Migrar o PostgreSQL para o Amazon Redshift como Data Warehouse (OLAP) para otimizar consultas analÃ­ticas complexas e de alta performance.

[ ] OrquestraÃ§Ã£o: Implementar o AWS Step Functions ou Apache Airflow (MWAA) para orquestrar e agendar o pipeline.