

# Projeto de Engenharia de Dados: ETL de Fundos de Investimento da CVM com Spark e AWS simulado.

Este projeto √© um pipeline de dados completo para extrair, transformar e carregar (ETL) os informes di√°rios de fundos de investimento brasileiros, disponibilizados publicamente pela CVM.

## üìú Sobre o Projeto
O projeto nasceu da uni√£o de duas √°reas de grande interesse: o mercado financeiro e a engenharia de dados. O objetivo principal foi construir um pipeline de dados robusto e escal√°vel para processar um grande volume de informa√ß√µes di√°rias dos fundos de investimento brasileiros ao longo do per√≠odo de 2022 at√© os dias atuais, disponibilizadas pela Comiss√£o de Valores Mobili√°rios (CVM).

O pipeline automatiza todo o processo, desde o download dos dados brutos at√© a disponibiliza√ß√£o em um banco de dados relacional, pronto para an√°lises e visualiza√ß√µes.

## üèóÔ∏è Arquitetura da Solu√ß√£o
A solu√ß√£o foi desenhada em um fluxo de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) e totalmente conteinerizada com Docker para garantir portabilidade e um ambiente de desenvolvimento isolado.

### 1. Extra√ß√£o (Extraction)
Fonte de Dados: Informes di√°rios da CVM (valor da cota, Patrim√¥nio L√≠quido, aplica√ß√µes e resgates) a partir de 2022.

Tecnologia: Scripts em Python utilizando as bibliotecas requests para o download dos arquivos e boto3 para a comunica√ß√£o com o S3.

Armazenamento Bruto (Raw): Os dados s√£o salvos em um bucket no Amazon S3, que √© simulado localmente atrav√©s do LocalStack.

### 2. Transforma√ß√£o (Transformation)
Desafio: Processar um grande volume de dados, que ultrapassa 22 milh√µes de registros.

Framework: Apache Spark (via PySpark) foi escolhido por sua capacidade de processamento distribu√≠do e alta performance.

Limpeza e padroniza√ß√£o dos dados.

Tratamento de valores nulos e tipos de dados.

Enriquecimento com novas m√©tricas, como o c√°lculo de PnL (Profit and Loss) e varia√ß√£o di√°ria da cota.

### 3. Carregamento (Loading)
Destino: Os dados limpos e processados s√£o carregados em um banco de dados PostgreSQL.

Prop√≥sito: Disponibilizar os dados estruturados para consumo por ferramentas de BI, dashboards ou outras aplica√ß√µes anal√≠ticas.

## üõ†Ô∏è Tecnologias Utilizadas

-  Python:	Linguagem principal para scripts de extra√ß√£o e transforma√ß√£o.
-  Apache Spark:	Framework de processamento de dados distribu√≠do para a etapa de transforma√ß√£o.
-  PostgreSQL:	Banco de dados relacional para armazenar os dados tratados.
-  Amazon S3 (LocalStack):	Armazenamento dos dados brutos (Data Lake).
-  Docker & Docker Compose:	Conteineriza√ß√£o e orquestra√ß√£o de toda a infraestrutura do projeto.


## üíª Estrutura de Pastas

```bash
ETL_CVM/
‚îú‚îÄ‚îÄ app/                 # Scripts principais do projeto
‚îú‚îÄ‚îÄ jars/                # Bibliotecas e drivers Java para Spark
‚îú‚îÄ‚îÄ notebooks/           # Notebooks para an√°lise e testes
‚îú‚îÄ‚îÄ sql/                 # Scripts SQL para banco de dados
‚îú‚îÄ‚îÄ src/                 # C√≥digo fonte do ETL
‚îú‚îÄ‚îÄ docker-compose.yml   # Orquestra√ß√£o Docker
‚îú‚îÄ‚îÄ Arquitetura ETL.png  # Diagrama da arquitetura ETL
‚îú‚îÄ‚îÄ README.md            # Documenta√ß√£o do projeto
‚îî‚îÄ‚îÄ .gitignore           # Arquivos ignorados pelo Git
```

## üöÄ Como Executar o Projeto
Siga os passos abaixo para executar todo o pipeline de ETL localmente.

Pr√©-requisitos
Antes de come√ßar, garanta que voc√™ tenha as seguintes ferramentas instaladas:

- Git
- Docker

Passo a Passo
1. Clone o Reposit√≥rio


```bash
git clone https://github.com/jonathannovs/ETL_CVM.git
cd ETL_CVM
```

2. Inicie os Cont√™ineres
Este comando ir√° construir e iniciar todos os servi√ßos definidos no docker-compose.yml (Spark, PostgreSQL, etc.) em segundo plano.
```bash
docker-compose up -d
```

3. Instale as Depend√™ncias Python
Acesse o cont√™iner do Spark Master para instalar as bibliotecas Python necess√°rias.
```bash
docker exec -it spark-master pip install -r /app/requirements.txt
```

4. Execute o Pipeline ETL
Execute o script principal main.py usando spark-submit. Este comando submete a aplica√ß√£o para o cluster Spark.
```bash
docker exec -it spark-master spark-submit \
  --jars /opt/bitnami/spark/jars-custom/postgresql-42.6.0.jar \
  --driver-class-path /opt/bitnami/spark/jars-custom/postgresql-42.6.0.jar \
  --master spark://spark-master:7077 \
  /app/main.py
```

O pipeline come√ßar√° a extrair, transformar e carregar os dados. O processo pode levar alguns minutos dependendo do volume de dados e da performance da sua m√°quina.

5. Verifique os Dados no PostgreSQL
Ap√≥s a execu√ß√£o do pipeline, voc√™ pode se conectar ao banco de dados para verificar se os dados foram carregados corretamente.
```bash SQL
SELECT * FROM cvm.fundos LIMIT 10;
```

## üîÆ Pr√≥ximos Passos
O roadmap futuro deste projeto inclui a migra√ß√£o da solu√ß√£o local para uma arquitetura 100% serverless na nuvem AWS, visando maior escalabilidade, resili√™ncia e automa√ß√£o.

[ ] Data Lake: Utilizar o Amazon S3 como Data Lake definitivo para os dados brutos e processados.

[ ] Processamento: Substituir o cluster Spark local por AWS Glue ou Amazon EMR para processamento distribu√≠do sob demanda.

[ ] Data Warehouse: Migrar o PostgreSQL para o Amazon Redshift como Data Warehouse (OLAP) para otimizar consultas anal√≠ticas complexas e de alta performance.

[ ] Orquestra√ß√£o: Implementar o AWS Step Functions ou Apache Airflow (MWAA) para orquestrar e agendar o pipeline.